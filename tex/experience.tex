%! TEX root = ../cv.tex

\section{\sc Experience}

{\bf \href{https://www.apple.com}{Apple}}, California, Unitied States
\begin{itemize}
    \item[] Senior Machine Learning Research Engineer in \href{https://machinelearning.apple.com}{AI/ML} \hfill Aug 2022 -- present
    \item[] Products:
        \begin{itemize}
            \item \href{https://www.apple.com/newsroom/2025/06/apple-intelligence-gets-even-more-powerful-with-new-capabilities-across-apple-devices/}{Live translation in Messages, FaceTime, and Phone}
            \item \href{https://support.apple.com/guide/iphone/translate-text-voice-and-conversations-iphd74cb450f/ios}{Translate app}
            \item \href{https://support.apple.com/en-in/guide/iphone/iphab4dcff1d/ios}{System-wide translation}
            \item \href{https://support.apple.com/guide/iphone/browse-the-web-iph1fbef4daa/ios}{Safari translation}
            \item \href{https://support.apple.com/en-in/guide/iphone/iph83aad8922/ios}{Siri translation}
        \end{itemize}
\end{itemize}

{\bf \href{http://research.baidu.com/}{Baidu Research}}, California, Unitied States
\begin{itemize}
    \item[] Senior Research Scientist in Institute of Deep Learning \hfill Nov 2021 -- Jun 2022
    \item[] Advisor: \href{https://web.engr.oregonstate.edu/~huanlian/}{Prof. Liang Huang}
    \item[] Research:
    \begin{itemize}
        \item A$^3$T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing~\cite{tmp-a3t} \\
            It reconstructs masked acoustic signals with text input and acoustic-text alignment during training, then the pretrained model can generate high quality reconstructed spectrogram, which can be applied to the speech editing and unseen speaker TTS directly.
        \item PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit~\cite{tmp-paddlespeech} \\
            I led the Paddle NLP team to demonstrate an all-in-one speech toolkit and wrote a paper that won \href{https://2022.naacl.org/blog/best-demo-award/}{the best demo award at NAACL}. PaddleSpeech provides an easy-to-use command-line interface and a clear code structure. It also achieves state-of-the-art performance on various speech datasets and implements the most popular methods.
        \item Prefix-finetuning for Simultaneous Machine Translation \\
            It proposes an efficient method to enable the full-sentence NMT model to achieve state-of-the-art simultaneous translation performance by a quick finetuning on an optimized prefix-to-prefix dataset.
            It also proposes an efficient and simple method for boosting lagging and translation quality trade-off by alleviating the train-test mismatch.
    \end{itemize}
\end{itemize}

\vspace{16pt}
{\bf \href{https://www.osu.edu/}{The Ohio State University}}, Ohio, Unitied States
\begin{itemize}
    \item[] Postdoc in \href{https://linguistics.osu.edu/}{Linguistics} Funded by Facebook \hfill Nov 2019 -- Oct 2021
    \item[] Advisor: \href{https://u.osu.edu/white.1240/}{Prof. Michael White}
    \item[] Research:
    \begin{itemize}
        \item Building Adaptive Acceptability Classifiers for Neural NLG~\cite{batra-etal-2021-building} \\
        It proposes a novel framework to train models to classify acceptability of responses generated by natural language generation (NLG) models, improving upon existing sentence transformation and model-based approaches.
        \item Leveraging Large Pretrained Models for {W}eb{NLG} 2020~\cite{li-etal-2020-leveraging-large} \\
        It reports experiments on finetuning large pretrained models to realize resource description framework (RDF) triples to natural language.
        \item Self-Training for Compositional Neural NLG in Task-Oriented Dialogue~\cite{li-white-wecnlp-2020, li-etal-2021-self} \\
        It shows that self-training enhanced with constrained decoding yields large gains in data efficiency on datasets that employ compositional meaning representations.
        In particular, the experiments indicate that self-training with constrained decoding can enable sequence-to-sequence models to achieve satisfactory quality using vanilla decoding with 5 to 10 times less data than with ordinary supervised baseline; moreover, by leveraging pretrained models, data efficiency can be increased further to 50 times.
        The end result is an approach that makes it possible to achieve acceptable performance on compositional NLG tasks using hundreds rather than tens of thousands of training samples.
        \item Neural NLG for \href{https://aclanthology.org/L16-1273/}{Methodius}~\cite{stevens-guille-etal-2020-neural, maskharashvili-etal-2021-neural} \\
        It shows that discourse relation relations significantly improve NLG when data is limited.
    \end{itemize}
\end{itemize}

{\bf \href{https://ai.tencent.com/ailab/en/index/}{Tencent AI Lab}}, Shenzhen, P.R. China
\begin{itemize}
    \item[] Research Intern in the NLP Group \hfill Jul 2017 -- May 2019
    \item[] Advisor: \href{https://lemaoliu.github.io/homepage/}{Dr. Lemao Liu}
    \item[] Research:
    \begin{itemize}
        \item Regularized Context Gates on Transformer for Machine Translation~\cite{li-etal-2020-regularized} \\
        It proposes a gate mechanism to control source and target contexts for the state of the art translation system.
        In addition, it proposes an effective method to regularize the context gates to reduce the bias of context gates learned from scratch.
        \item On the Word Alignment from Neural Machine Translation~\cite{li-etal-2019-word} \\
        It reveals attention may not capture word alignment and proposes two effective methods to induce word alignment from general NMT models.
        It also shows NMT indeed learn excellent word alignment regarding its own translation although its alignment regarding reference seems weaker than statistical aligners.
        Finally, it demonstrates word alignment errors have adverse effect on translation quality.
        \item Target Foresight based Attention for Neural Machine Translation~\cite{li-etal-2018-target, li2021attending} \\
        It proposes a novel attention mechanism by predicting the target foresight word with an auxiliary network to enhance the attention model and achieves significant improvement both on translation and alignment performance.
        \item Understanding and Improving Hidden Representations for Neural Machine Translation~\cite{li-etal-2019-understanding-improving} \\
        It proposes methods to regularize hidden representations and interpret what hidden representations learn from translation.
    \end{itemize}
\end{itemize}

{\bf \href{https://www.ee.cuhk.edu.hk/}{Department of Electronic Engineering}}, The Chinese University of Hong Kong
\begin{itemize}
    \item[] Ph.D. Student in Robotics and Perception Laboratory \hfill Aug 2015 -- Jul 2019
    \item[] Advisor: \href{http://www.ee.cuhk.edu.hk/~qhmeng/about.html}{Prof. Max Q.-H. Meng}
    \item[] Research:
    \begin{itemize}
        \item Efficient Object Search With Belief Road Map Using Mobile Robot~\cite{wang2018efficient} \\
        It formulates the object search problem as a Partially Observable Markov Decision Process utilizing semantic information.
        \item Cuffless Blood Pressure Estimation using Recurrent Neural Network~\cite{lo2017continuous} \\
        Considering electrocardiogram (ECG) and photoplethysmography (PPG) signals are time series, the feed-forward neural networks have difficulties to handle this type of data.
        From these observations, a novel structure of recurrent neural network based on long short-term memory has been proposed to learn the mapping from ECG and PPG to blood pressure.
        \item Algorithms for Reducing Motion Artifacts in Photoplethysmography Signal~\cite{lo2017motion} \\
        The project aims at removing motion artifacts in PPG signal captured by wearable sensors so as to enhance the accuracy and credibility of continuous blood pressure estimation.
        Statistical signal processing techniques such as independent component analysis, dimensionality reduction and matrix factorization have been involved in the project.
        \item Robotic Path Planning with Obstacles~\cite{wang2016improved} \\
        The project takes obstacles' information into rapidly-exploring random tree algorithm, making the two dimensional robotic path planing more efficient.
    \end{itemize}
    \item[] Teaching:
    \begin{itemize}
        \item Teaching Assistant of Medical Robotics \hfill Jan 2016 -- May 2016 \\
        Teaching kinematics of robot manipulator and fundamentals of robot control in surgical robotics, and introducing the novel robotic applications in medical care.
        \item Teaching Assistant of Biomedical Modeling \hfill Sep 2015 -- Dec 2016 \\
        Teaching basic concepts and fundamental techniques of mathematical modeling, and how the methodologies are used in biomedical modeling via various physiological case studies.
    \end{itemize}
\end{itemize}
