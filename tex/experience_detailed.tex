%! TEX root = ../cv.tex

\section{\sc Experience}

{\bf \href{http://research.baidu.com/}{Baidu Research}}, California, Unitied States
\begin{itemize}
    \item[] Senior Research Scientist in Institute of Deep Learning \hfill Nov 2021 -- present
    \item[] Advisor: \href{https://web.engr.oregonstate.edu/~huanlian/}{Prof. Liang Huang}
    \item[] Research:
    \begin{itemize}
        \item {\bf PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit}~\cite{tmp-paddlespeech} \\
            I led the Paddle NLP team to demonstrate an all-in-one speech toolkit and wrote a paper that has been accepted by NAACL Demo Track recently. PaddleSpeech provides an easy-to-use command-line interface and a clear code structure. It also achieves state-of-the-art performance on various speech datasets and implements the most popular methods.
        \item {\bf Efficient Training for Simultaneous Translation by Data Optimization}~[On Going] \\
            It proposes an efficient method to enable the full-sentence NMT model to achieve state-of-the-art simultaneous translation performance by a quick finetuning on an optimized prefix-to-prefix dataset.
        \item {\bf Forced Decoding for Simultaneous Translation}~[On Going] \\
            It proposes an efficient and simple method for boosting lagging and translation quality trade-off by alleviating the train-test mismatch.
    \end{itemize}
\end{itemize}

{\bf \href{https://www.osu.edu/}{The Ohio State University}}, Ohio, Unitied States
\begin{itemize}
    \item[] Postdoc in \href{https://linguistics.osu.edu/}{Linguistics} \hfill Nov 2019 -- Oct 2021
    \item[] Advisor: \href{https://u.osu.edu/white.1240/}{Prof. Michael White}
    \item[] Funded by Facebook
    \item[] Research:
    \begin{itemize}
        \item {\bf Building Adaptive Acceptability Classifiers for Neural NLG}~\cite{batra-etal-2021-building} \\
            It proposes a novel framework to train models to classify acceptability of responses generated by natural language generation (NLG) models, improving upon existing sentence transformation and model-based approaches.
        \item {\bf Leveraging Large Pretrained Models for {W}eb{NLG} 2020}~\cite{li-etal-2020-leveraging-large} \\
            It reports experiments on finetuning large pretrained models to realize resource description framework (RDF) triples to natural language.
            Our system is ranked first among 15 teams all over the world including them from Google, Amazon, Facebook, and Huawei.
        \item {\bf Self-Training for Compositional Neural NLG in Task-Oriented Dialogue}~\cite{li-white-wecnlp-2020, li-etal-2021-self} \\
            It shows that self-training enhanced with constrained decoding yields large gains in data efficiency on datasets that employ compositional meaning representations.
            In particular, the experiments indicate that self-training with constrained decoding can enable sequence-to-sequence models to achieve satisfactory quality using vanilla decoding with 5 to 10 times less data than with ordinary supervised baseline; moreover, by leveraging pretrained models, data efficiency can be increased further to 50 times.
            The end result is an approach that makes it possible to achieve acceptable performance on compositional NLG tasks using hundreds rather than tens of thousands of training samples.
        \item {\bf Neural NLG for \href{https://aclanthology.org/L16-1273/}{Methodius}}~\cite{stevens-guille-etal-2020-neural, maskharashvili-etal-2021-neural} \\
            It shows that discourse relation relations significantly improve NLG when data is limited.
    \end{itemize}
\end{itemize}

{\bf \href{https://ai.tencent.com/ailab/en/index/}{Tencent AI Lab}}, Shenzhen, P.R. China
\begin{itemize}
    \item[] Research Intern in the NLP Group \hfill Jul 2017 -- May 2019
    \item[] Research:
    \begin{itemize}
        \item {\bf Regularized Context Gates on Transformer for Machine Translation}~\cite{li-etal-2020-regularized} \\
            It proposes a gate mechanism to control source and target contexts for the state of the art translation system.
            In addition, it proposes an effective method to regularize the context gates to reduce the bias of context gates learned from scratch.
        \item {\bf On the Word Alignment from Neural Machine Translation}~\cite{li-etal-2019-word} \\
            It reveals attention may not capture word alignment and proposes two effective methods to induce word alignment from general NMT models.
            It also shows NMT indeed learn excellent word alignment regarding its own translation although its alignment regarding reference seems weaker than statistical aligners.
            Finally, it demonstrates word alignment errors have adverse effect on translation quality.
        \item {\bf Target Foresight based Attention for Neural Machine Translation}~\cite{li-etal-2018-target, li2021attending} \\
            It proposes a novel attention mechanism by predicting the target foresight word with an auxiliary network to enhance the attention model and achieves significant improvement both on translation and alignment performance.
        \item {\bf Understanding and Improving Hidden Representations for Neural Machine Translation}~\cite{li-etal-2019-understanding-improving} \\
            It proposes methods to regularize hidden representations and interpret what hidden representations learn from translation.
    \end{itemize}
\end{itemize}

{\bf \href{https://www.ee.cuhk.edu.hk/}{Department of Electronic Engineering}}, The Chinese University of Hong Kong
\begin{itemize}
    \item[] Ph.D. Student in Robotics and Perception Laboratory \hfill Aug 2015 -- Jul 2019
    \item[] Advisor: \href{http://www.ee.cuhk.edu.hk/~qhmeng/about.html}{Prof. Max Q.-H. Meng}
    \item[] Research:
    \begin{itemize}
        \item {\bf Efficient Object Search With Belief Road Map Using Mobile Robot}~\cite{wang2018efficient} \\
            It formulates the object search problem as a Partially Observable Markov Decision Process utilizing semantic information.
        \item {\bf Cuffless Blood Pressure Estimation using Recurrent Neural Network}~\cite{lo2017continuous} \\
            Considering electrocardiogram (ECG) and photoplethysmography (PPG) signals are time series, the feed-forward neural networks have difficulties to handle this type of data.
            From these observations, a novel structure of recurrent neural network based on long short-term memory has been proposed to learn the mapping from ECG and PPG to Blood Pressure.
        \item {\bf Algorithms for Reducing Motion Artifacts in Photoplethysmography Signal}~\cite{lo2017motion} \\
            The project aims at removing motion artifacts in PPG signal captured by wearable sensors so as to enhance the accuracy and credibility of continuous blood pressure estimation.
            Statistical signal processing techniques such as independent component analysis, dimensionality reduction and matrix factorization have been involved in the project.

        \item {\bf Robotic Path Planning with Obstacles}~\cite{wang2016improved} \\
            The project takes obstacles' information into rapidly-exploring random tree algorithm, making the two dimensional robotic path planing more efficient.
    \end{itemize}
    \item[] Teaching:
    \begin{itemize}
        \item Teaching Assistant of Medical Robotics \hfill Jan 2016 -- May 2016 \\
            Teaching kinematics of robot manipulator and fundamentals of robot control in surgical robotics, and introducing the novel robotic applications in medical care.
        \item Teaching Assistant of Biomedical Modeling \hfill Sep 2015 -- Dec 2016 \\
            Teaching basic concepts and fundamental techniques of mathematical modeling, and how the methodologies are used in biomedical modeling via various physiological case studies.
    \end{itemize}
\end{itemize}
